---
layout: post
title: 机器学习经典算法优缺点总结
category: 机器学习
tags: Machine Learning
keywords: 算法、机器学习
description: 
---

<embed src="http://www.xiami.com/widget/0_1773240231/singlePlayer.swf" type="application/x-shockwave-flash" width="257" height="33" wmode="transparent"></embed>

---

#**欢迎在楼底留言补充，我进行完善**

---

#1.决策树:判别模型，多分类与回归，正则化的极大似然估计

##特点：

>适用于小数据集

##优点：　
>计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；

##缺点：
>容易过拟合（后续出现了随机森林，减小了过拟合现象）,使用剪枝来避免过拟合；

##适用数据范围：
>数值型和标称型

##CART分类与回归树:
>决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数据集生成的决策树的拓展形。决策树回归方法，采用切分点与切分变量来计算的损失来估计函数。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。

###优点:
>非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树,产生的分类规则易于理解，准确率较高。  

###缺点：
>在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。

---

#2.随机森林：判别模型，多分类与回归，正则化的极大似然估计，Bagging，Random Future
##特点：
>准确率可以和Adaboost相媲美，对错误和离群点更鲁棒。准确率依赖于个体分类器的实力和它们之间的依赖性。理想情况是保持个体分类器的能力而不提高它们的相关性。对每次划分所考虑的属性数很敏感。通常选取logn2+1个属性，其中n是数据集的实例数。（一个有趣的观察是，使用单个随机选择的属性可能导致很好的准确率，常常比使用多个属性更高。）

##优点：
>不易过拟合，可能比Bagging和Boosting更快。由于在每次划分时只考虑很少的属性，因此它们在大型数据库上非常有效。有很好的方法来填充缺失值，即便有很大一部分数据缺失，仍能维持很高准确度。给出了变量重要性的内在估计，对于不平衡样本分类，它可以平衡误差。可以计算各实例的亲近度，对于数据挖掘、检测离群点和数据可视化非常有用。

##缺点：
>在某些噪声较大的分类和回归问题上会过拟合。对于有不同级别的属性的数据，级别划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产生的属性权值是不可信的。

---

#3.k-means：判别模型，多分类
##特点：
>并一定能得到全局最优解（依赖于初始点选取），所以常用多次运行，取最优，假设了均方误差为计算群组分散度的最佳参数

##优点：
>简单快速，复杂度为O(nkt),n为样本数，k为类别数，t为迭代数

##缺点：
>只对簇的平均值被定义下才能被使用，不适合某些分类属性，虚实线给定簇数K，对初值敏感，不适合发现大小差别很大的簇，对噪声、孤立点敏感（对平均值产生极大影响）
 
---

#4.KNN：判别模型，多分类与回归
##特点：
>不具有显示的学习过程，通过多数表决方式进行预测，k值选择、距离度量、分类决策规则是K近邻法的三要素 

##优点：
>简单，分类与回归均可操作，可用于非线性分类，复杂度为O(n)，对outlier不敏感

##缺点：
>K需预先设定，对大小不平衡的数据易偏向大容量数据

##常用算法：
>kd树：对x的K个特征，一个一个做切分，使得每个数据最终都在切分点上（中位数），对输入的数据搜索kd树，找到K近邻   

---

#5.EM：含隐藏变量的概率模型，使用概率模型参数估计
##特点：
>E:给定参数与观测数据下对未观测数据的条件概率分布的期望  
>M：求使条件概率分布期望最大下的参数值

##优点：
>比K-means稳定、准确

##缺点：
>计算复杂且收敛慢，依赖于初始参数假设

---

#6.线性回归
##特点：
>解析解

##优点：
>简单，存在解析解

##缺点：
>对复杂数据拟合不好，欠拟合

---

#7.LogReg：对数线性模型
##特点：
>模型源自于逻辑斯蒂分布优化算法有改进的迭代尺度法、梯度下降法、拟牛顿法

##优点：
>简单，计算量小，存储资源低

##缺点：
>欠拟合，精度不高

---

#8.朴素贝叶斯：生成模型，
##特点：
>使用先验知识得到后验概率，由期望风险最小化得到后验概率最大化。假设条件独立，条件不独立就变成贝叶斯网络了

##优点：
>小规模数据集表现好，适合多分类

##缺点：
>需要条件独立假设，会牺牲一定准确率，分类性能不一定高

---

#9.Apriori:两阶段频集思想，递推（关联规则）
##特点：
>1频度→支持度→2频度→支持度→...，每次删除支持度小于摸个阀值的点，最终返回各个频集

##优点：
>易编码实现

##缺点：
>大数据上速度较慢，候选集每次产生过多，未排除不应该参与计算支持度的点.  
>每次都需要计算支持度，需对全部记录扫描，需要很大I/O负载

---

#10.Boosting
##特点：
>通过改变样本权值进行学习，将最终的多个分类器根据性能进行组合

##优点：
>低泛化误差，以实现，分类准确率高，无太多参数需要调节

##缺点：
>对outlier敏感

---

#11.GBDT(MART)：回归树
##特点：
>有两个版本：一个是残差版本，另一个是Gradient版本（这个版本更广泛）

##优点：
>非线性与线性均可，不易过拟合

##缺点：
> 
---

#12.SVM：
##特点：
>将低维空间映射到高维空间，实现线性可分

##优点：
>可实现非线性分类，可用于分类与回归，低泛化误差，易解释

##缺点：
>对核函数以及参数敏感

---

#13.神经网络
##特点：
>模拟人脑构造，构造神经元

##优点：
>(BP)很强的分线性拟合能力，学习规则简单，很强的鲁棒性，具有记忆能力、自学能力,误差反向传播，并行性好  
>(RBF)唯一最佳逼近特性，无局部最小问题，前反馈网络中RBF网络完成映射功能最优，分类能力好，收敛性比BP快非常多

##缺点：
>没能力解释自己的推理过程及依据，数据不充分时，将无法工作，初值较敏感（使用AUTO-Encoder）

---

#14.隐式马尔科夫（HMM）
##特点：
>隐马尔可夫模型是一个双重随机过程----具有一定状态数的隐马尔可夫链和显示随机函数集。  
>HMM是一种有向图  
>HMM对转移概率和表现概率直接建模，统计共现概率。  
>针对以下三个问题，人们提出了相应的算法  
>*1 评估问题： 前向算法  
>*2 解码问题： Viterbi算法  
>*3 学习问题： Baum-Welch算法(向前向后算法)

##优点：
>解决了标注问题 

##缺点：
>做了齐次马尔科夫假设及观测股利性假设，可能出现标记偏置

---

#15.条件随机场（CRF）
##特点：
>CRF是一种判别式模型,CRF是一种无向图

##优点：
>CRF是在全局范围内统计归一化的概率，是全局最优的解。解决了MEMM中标注偏置的问题。  
>CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样）  
>与MEMM比较:由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。  
>与ME比:CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。

##缺点：
>训练代价大、复杂度高