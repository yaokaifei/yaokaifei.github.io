---
layout: post
title: 机器学习经典算法优缺点总结
category: 机器学习
tags: Machine Learning
keywords: 算法、机器学习
description: 
---

<embed src="http://www.xiami.com/widget/0_1773240231/singlePlayer.swf" type="application/x-shockwave-flash" width="257" height="33" wmode="transparent"></embed>


#1.决策树:判别模型，多分类与回归，正则化的极大似然估计

##特点：

    适用于小数据集

##优点：　
    计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；

##缺点：
    容易过拟合（后续出现了随机森林，减小了过拟合现象）,使用剪枝来避免过拟合；

##适用数据范围：
    数值型和标称型

##CART分类与回归树:

    决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数据集生成的决策树的拓展形。
    决策树回归方法，采用切分点与切分变量来计算的损失来估计函数。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。
    分类树是使用树结构算法将数据分成离散类的方法。

###优点:
	非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树,产生的分类规则易于理解，准确率较高。
###缺点：
    在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。
此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。
<br>

#2.随机森林：判别模型，多分类与回归，正则化的极大似然估计，Bagging，Random Future
##特点：
	准确率可以和Adaboost相媲美，对错误和离群点更鲁棒。准确率依赖于个体分类器的实力和它们之间的依赖性。
    理想情况是保持个体分类器的能力而不提高它们的相关性。对每次划分所考虑的属性数很敏感。通常选取logn2+1个属性，其中n是数据集的实例数。
    （一个有趣的观察是，使用单个随机选择的属性可能导致很好的准确率，常常比使用多个属性更高。）
##优点：
	不易过拟合，可能比Bagging和Boosting更快。由于在每次划分时只考虑很少的属性，因此它们在大型数据库上非常有效。
    有很好的方法来填充缺失值，即便有很大一部分数据缺失，仍能维持很高准确度。
	给出了变量重要性的内在估计，对于不平衡样本分类，它可以平衡误差。
	可以计算各实例的亲近度，对于数据挖掘、检测离群点和数据可视化非常有用。
##缺点：
	在某些噪声较大的分类和回归问题上会过拟合。
	对于有不同级别的属性的数据，级别划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产生的属性权值是不可信的。


#3.k-means：判别模型，多分类
##特点：
    并一定能得到全局最优解（依赖于初始点选取），所以常用多次运行，取最优，假设了均方误差为计算群组分散度的最佳参数

##优点：
    简单快速，复杂度为O(nkt),n为样本数，k为类别数，t为迭代数

##缺点：
    只对簇的平均值被定义下才能被使用，不适合某些分类属性，虚实线给定簇数K，对初值敏感，不适合发现大小差别很大的簇，对噪声、孤立点敏感（对平均值产生极大影响）
 

#4.KNN：判别模型，多分类与回归
##特点：
    

##优点：
    简单，分类与回归均可操作，可用于非线性分类，复杂度为O(n)，对outlier不敏感

##缺点：
    K需预先设定，对大小不平衡的数据易偏向大容量数据
##常用算法：
    kd树：对x的K个特征，一个一个做切分，使得每个数据最终都在切分点上（中位数），对输入的数据搜索kd树，找到K近邻   

#5.EM：含隐藏变量的概率模型，使用概率模型参数估计
##特点：

##优点：
    比K-means稳定、准确

##缺点：
    计算复杂且收敛慢，依赖于初始参数假设
#
##特点：

##优点：

##缺点：

#
##特点：

##优点：

##缺点：

从算法的表现上来说，它并不保证一定得到全局最优解，最终解的质量很大程度上取决于初始化的分组。由于该算法的速度很快，因此常用的一种方法是多次运行k平均算法，选择最优解。
k平均算法的一个缺点是，分组的数目k是一个输入参数，不合适的k可能返回较差的结果。另外，算法还假设均方误差是计算群组分散度的最佳参数。
优点：算法速度很快
缺点是，分组的数目k是一个输入参数，不合适的k可能返回较差的结果。
k-means算法的
优点：
　　（1）k-means算法是解决聚类问题的一种经典算法，算法简单、快速。
　　（2）对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k<<n。这个算法通常局部收敛。
　　（3）算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。
缺点：
　　（1）k-平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合。
　　（2）要求用户必须事先给出要生成的簇的数目k。
　　（3）对初值敏感，对于不同的初始值，可能会导致不同的聚类结果。
　　（4）不适合于发现非凸面形状的簇，或者大小差别很大的簇。
　　（5）对于"噪声"和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。
2. 基于层次的聚类：
　　自底向上的凝聚方法，比如AGNES。
　　自上向下的分裂方法，比如DIANA。
3. 基于密度的聚类：
　　DBSACN,OPTICS,BIRCH(CF-Tree),CURE.
4. 基于网格的方法：
　　STING, WaveCluster.
5. 基于模型的聚类：
EM,SOM,COBWEB.
k-Means
	优点：容易实现
	缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢。
	适用数据类型：数值型数据

K最近邻分类算法（KNN）
缺点：
	K值需要预先设定，而不能自适应
	当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。
该算法适用于对样本容量比较大的类域进行自动分类。
KNN算法的
优点：
　　1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归；
　　2. 可用于非线性分类；
　　3. 训练时间复杂度为O(n)；
　　4. 准确度高，对数据没有假设，对outlier不敏感；
缺点：
　　1. 计算量大；
　　2. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；
3. 需要大量的内存；
kNN
	特点：k值的选择会对k近邻法的结果产生重大影响
	优点：精度高、对异常值不敏感、无数据输入假定
	缺点：计算复杂度高、空间复杂度高
	适用数据范围：数值型和标称型

EM最大期望算法
EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量 。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。
EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。

线性回归
优点：
实现简单，计算简单；
缺点：
不能拟合非线性数据；

Logistic回归
优点：
1、实现简单；
2、分类时计算量非常小，速度很快，存储资源低；
缺点：
1、容易欠拟合，一般准确度不太高
2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；
Logistic回归
	优点：计算代价不高，易于理解和实现。
	缺点：容易欠拟合，分类的精度可能不高。
	适用数据范围：数值型和标称型

朴素贝叶斯算法：
朴素贝叶斯假设是约束性很强的假设，假设特征条件独立，但朴素贝叶斯算法简单，快速，具有较小的出错率。在朴素贝叶斯的应用中，主要研究了电子邮件过滤以及文本分类研究。　　
朴素贝叶斯的优点：
对小规模的数据表现很好，适合多分类任务，适合增量式训练。
缺点：
对输入数据的表达形式很敏感。
朴素贝叶斯
	优点：学习和预测的效率高，且易于实现；在数据较少的情况下仍然有效，可以处理多类别问题。
	缺点：分类性能不一定很高；条件假设独立会使朴素贝叶斯变得简单，但有时会牺牲一定的分类准确率。
	适用数据范围：标称型数据

Apriori算法
Apriori算法是一种挖掘关联规则的算法，用于挖掘其内含的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法 。
算法缺点：
	在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；
	每次计算项集的支持度时，都对数据库中    的全部记录进行了一遍扫描比较，需要很大的I/O负载。
Apriori算法
	优点：易编码实现
	缺点：在大数据集上可能较慢
	适用数据类型：数值型或者标称型数据

Boosting算法的
优点：
　　低泛化误差；
　　容易实现，分类准确率较高，没有太多参数可以调；
缺点：
对outlier比较敏感；
Boosting
	优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。
	缺点：对离群点敏感。
	适用数据范围：数值型和标称型

AdaBoost 
Adaboost是一种迭代算法， 
目前AdaBoost算法广泛的应用于人脸检测、目标识别等领域。


SVM支持向量机
支持向量机是一种基于分类边界的方法。
支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。
SVM在解决小样本、非线性及高维模式识别问题中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。
	在面对诸如存在缺失值、变量数多等问题时CART 显得非常稳健。
SVM算法优点：
可用于线性/非线性分类，也可以用于回归；
低泛化误差；
容易解释；
计算复杂度较低；
缺点：
对参数和核函数的选择比较敏感；
原始的SVM只比较擅长处理二分类问题；

PageRank算法:
优点：是一个与查询无关的静态算法，所有网页的PageRank值通过离线计算获得；有效减少在线查询时的计算量，极大降低了查询响应时间。
不足：人们的查询具有主题特征，PageRank忽略了主题相关性，导致结果的相关性和主题性降低；另外，PageRank有很严重的对新网页的歧视。
Topic-Sensitive PageRank（主题敏感的PageRank）
优点：根据用户的查询请求和相关上下文判断用户查询相关的主题（用户的兴趣）返回查询结果准确性高。
不足：没有利用主题的相关性来提高链接得分的准确性。
Hilltop
优点：相关性强，结果准确。
不足：专家页面的搜索和确定对算法起关键作用，专家页面的质量决定了算法的准确性，而专家页面 的质量和公平性难以保证；忽略了大量非专家页面的影响，不能反应整个Internet的民意；当没有足够的专家页面存在时，返回空，所以Hilltop适合对于查询排序进行求精。

