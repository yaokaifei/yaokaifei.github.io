---
layout: post
title: 浅谈稀疏学习
category: 机器学习
tags: Machine Learning
keywords: 算法、机器学习
description: 
---

<embed src="http://www.xiami.com/widget/0_2112850/singlePlayer.swf" type="application/x-shockwave-flash" width="257" height="33" wmode="transparent"></embed>

---

##**欢迎在楼底留言补充，我进行完善**

---
>###稀疏学习：大部分参数都设置为0的学习过程，加速计算过程，在大数据今天的特别有应用场景，这里对约束使用不同的范数可以有不同的效果

#1.范数
###我们先来看看范数：
>1.L0范数：向量中非零元素的个数  
2.L1范数：向量中各元素的绝对值和，这里L1为L0的最优凸近似  
（另一种解释：任何一个规则化算子，若其在Wi=0上不可微，并且可以分解为求和形式，则可以实现稀疏解）  
3.L2范数：向量中各元素的平方和  
4.核范数：矩阵奇异值的和  
5.迹范数  
6.Frobenius    
  
![2](/public/img/posts/机器学习/浅谈稀疏学习/2.png)

###效果
>1.L0范数与L1范数效果一样在实际算法中将大部分参数设置为0，一定程度上等价，而L0很难求解，L1可以借助L2做一个迭代的算法。  
2.L2范数则实际上的效果为岭回归

###为什么L1必须存在于稀疏学习的约束条件中
>因为从范数的角度看，是否存在最优解：当范数q小于1时非凸，非凸将可能存在局部最优解，而L1既是凸函数又是L0一定意义上稀疏解的等价形式，所以L1将必然存在于稀疏学习的约束中

###实际应用
>弹性网回归学习算法：实际应用将L1与L2结合

![1](/public/img/posts/机器学习/浅谈稀疏学习/1.PNG)